\section{Evaluation and phenomena}

Textual entailment offers a good framework for testing whether a
system performs correct analyses and thus draws the right inferences
from a given text. For example, to test whether a system correctly
handles implicative verbs, one can use the \emph{premise} $p$ along with
the \emph{hypothesis} $h$ in \eqref{ex:imp-fact-nested} below. If the system
analyses the two sentences correctly, it should infer that $h$ holds.
While the most prominent forum using textual entailment is the 
Recognizing Textual Entailment (RTE) challenge \citep{dagan:rte2005},
the RTE datasets 
do not test the phenomena in which we are interested.
For example, in order to evaluate our system's ability to determine word meaning
in context, the RTE pair would have to specifically test word sense confusion by
having a word's context in the hypothesis be different from the context of the
premise.  However, this simply does not occur in the RTE corpora.  In order to
properly test our phenomena, we construct hand-tailored premises and hypotheses
based on real-world texts.


In this paper, we focus on three natural language phenomena and their
interaction: implicativity and factivity, word meaning, and coreference.
The first phenomenon, implicativity and factivity, is concerned with analyzing
the truth conditions of nested propositions.  For example, in the premise of
the entailment pair shown in example 
\eqref{ex:imp-fact-nested}, ``arrange that'' falls under the scope of 
``forget to'' and ``fail'' is under the scope of ``arrange that''.
Correctly recognizing nested propositions is necessary for preventing
false inferences such as the one
in example \eqref{ex:hope-build}.

\begin{example}\label{ex:imp-fact-nested}
\begin{itemize}
  \item[$p$:] Ed did not forget to arrange that Dave 
fail\footnote{Examples \eqref{ex:imp-fact-nested}
and \eqref{ex:imp-fact-hyper} and Figure \ref{fig:imp-sig} are based on examples by
\citet{maccartney:iwcs2009}}
  \item[$h$:] Dave failed
\end{itemize}
\end{example}

\begin{example}\label{ex:hope-build}
\begin{itemize}
  \item[$p$:] The mayor hoped to build a new stadium\footnote{Examples
\eqref{ex:hope-build}, \eqref{ex:prob-wordsense}, \eqref{ex:coref}, and
\eqref{ex:ws-coref} are modified versions of sentences from document wsj\_0126
from the Penn Treebank}
  \item[$h$*:] The mayor built a new stadium
\end{itemize}
\end{example}


For the second phenomenon, word meaning, we address paraphrasing and
hypernymy.  For example, in \eqref{ex:prob-wordsense} ``covering'' is a good
paraphrase for ``sweeping'' while ``brushing'' is not.

\begin{example}\label{ex:prob-wordsense}
\begin{itemize}
  \item[$p$:]   A stadium craze is \textbf{sweeping} the country
  \item[$h_1$:] A stadium craze is \textbf{covering} the country
  \item[$h_2$*:] A stadium craze is \textbf{brushing} the country
\end{itemize}
\end{example}

The third phenomenon is coreference, as illustrated in \eqref{ex:coref}.  For
this example, to correctly judge the hypothesis as entailed, it is necessary
to recognize that ``he'' corefers with ``Christopher'' and ``the new ballpark''
corefers with ``a replacement for Candlestick Park''.

\begin{example}\label{ex:coref}
\begin{itemize}
  \item[$p$:] George Christopher has been a critic of the plan to build a
  replacement for Candlestick Park. As a result, he won't endorse the new ballpark.
  \item[$h$:] Christopher won't endorse a replacement for Candlestick Park.
\end{itemize}
\end{example}

Some natural language phenomena are most naturally treated as categorial, while 
others are more naturally treated using weights or 
probabilities. In this paper, we treat implicativity and coreference
as categorial phenomena, while using a probabilistic approach to word
meaning. 

