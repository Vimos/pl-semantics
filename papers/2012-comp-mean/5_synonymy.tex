\section{Ambiguity in word meaning}

In order for our system to be able to make correct natural language inferences,
it must be able to handle paraphrasing.  For example, in order to license the
entailment pair in (\ref{ex:syn-hyp-pos}), the system must recognize that
``owns'' is a valid paraphrase for ``has'', and that a ``car'' is type of
``vehicle'':

\entpairex{ex:syn-hyp-pos}{Ed owns a car.}{Ed has a vehicle.}

We address this problem as described in Section~\ref{sec:interface}: 
We use distributional information to
generate inferences stating, for example,  that ``has'' can be substituted for
``owns''. This inference is weighted by the degree to which 
``owns'', in the context in which it is used in
(\ref{ex:syn-hyp-pos}), is similar to ``has''. To integrate these
inference rules with the logical form representations of sentences
like (\ref{ex:syn-hyp-pos}), we use the formalism introduced in
Section \ref{sec:interface}. We now describe how we instantiate it in
the current paper.

% KE: weird paragraph, sounds like we had not introduced the notion
% of inference rules based on distributional information
% before. Instead, point to section 3. 
% Perhaps the most natural fit for our system of projecting distributional
% information into logical forms is trying to generate inference rules to address
% lexical ambiguity.  For any natural language sentence $A$, a word $v$ in $A$ may
% be replaced by a synonym of $v$, $w$, resulting in a new sentence $A'$.  The
% degree to which $A'$ means the same thing as $A$ is determined by how well $w$
% fits the context of $v$ in $A$.  Thus, in order to capture the
% strength of our conviction that
% $A$ entails $A'$, we want to generate an inference rule stating that $v$ implies
% $w$ to the degree that that $w$ fits the context of $v$.


First, we generate a vector space $V$.  We have chosen to implement a very
simple bag-of-words vector space.  To ensure that the entries in the vector
space correspond to the predicates in our logical forms, we first lemmatize all
sentences in our corpus using the same lemmatization process as Boxer.
The features used by $V$ are the $N$ most frequent lemmas, excluding stopwords.  
Each lemma in the corpus is represented by a vector in $V$.  To calculate these
vectors, we count the number of times the lemma appears in the same sentence
as each feature, and then calculate the point-wise mutual information (PMI)
between the lemma and each feature.  The resulting PMI values for each feature
are used as the vector for the lemma.

As the \emph{similarity function} \simfunc on our vector space, we use cosine
similarity. For two vectors $\vec v$ and $\vec w$, their similarity is 
\[ \simfunc(\vec v, \vec w) = cosine(\vec v, \vec w) = \frac{\vec v \cdot \vec
w}{\|\vec v\|~\|\vec w\|}\]

Logical forms in our system are generated by Boxer, so our logical language
\loglang is the set of formulas that may be returned from Boxer.  Likewise, the
set of predicate symbols $\predsym{\loglang}$ are the predicates generated by
Boxer. Boxer's predicates, as represented by the {\tt pred} relation in Boxer's
Prolog output\footnote{See
\url{http://svn.ask.it.usyd.edu.au/trac/candc/wiki/DRSs} for the detailed
grammar of Boxer DRS output.}, consist of a word lemma and a token index
indicating the original token that generated that predicate.  
% lexical mapping maps to vectors, not words!
Our \emph{lexical mapping} function maps each predicate symbol to the
vector that represents the lemma portion of the predicate.

In order to assess the similarity between a word's context and a possible
replacement word, we must define a \textit{context mapping} that generates a
context from a predicate $P \in \predsym{\loglang}$ and a formula $G \in
\loglang$.  For the current paper we use the simplest possible
definition for $\kappa$, which ignores semantic relations. We define 
% We can define our context mapping as a function that maps $P$ to the set of
% vectors representing the other predicates in $G$ along with the relations that
% connect them.
% \begin{align*}
% \kappa(P,G) = \{ (r_i, \ell(Q)) ~| 
% &~Q \text{ is a predicate found in } G, \\
% &~r_i = \text{the relation connecting } P \text{ and } Q, \text{ and } \\
% &~Q \neq P \}
% \end{align*}
% However, in our current scenario, the only ``relation'' we use is the relation
% of being in the same sentence.  So, we are defining 
the context of $P$ as the
vectors of all predicates $Q$ that occur in the same sentence as $P$.
Since every predicate in a logical form returned by Boxer is indexed
with the sentence from which it was generated, we can define a simple context
mapping that defines a predicate's context solely in terms of the other
predicates generated by boxer for that sentence.
\begin{align*}
\kappa(P,G) = \{ (same\text{-}sentence, \ell(Q)) ~|
&~Q \text{ is a predicate found in } G, \\
&~Q\text{'s sentence index} = P\text{'s sentence index}, \text{ and } \\
&~Q \neq P \}
\end{align*}
Note that the only predicates $Q$ that are used are those derived from the
lemmas of words found in the text.  Meta-predicates representing relations such
as $agent$, $patient$, and $theme$ are not included.

The context mapping $\kappa$ computes a context for a predicate $P$
occurring in a formula $G$. Next we require a 
\textit{contextualization function} that uses the context returned by
$\kappa$ to compute a context-specific vector for $P$. Again we use
the simplest instantiation possible. Our contextualization
function just computes the sum of the vectors for each lemma in the context \[ \alpha(\vec v,
c) = \sum_{(r_i, \vec w_i) \in c} \vec w_i \]  Other, more complex
instantiations of $\kappa$ and $\alpha$ are possible. We comment on
thus further in Section~\ref{sec:future}. 

Based on these definitions, we compute the \textit{contextualized
inference projection} $\Pi^G_{\simfunc,\zeta,\ell}(P)$, the set of weighted
inference rules mapping predicate $P$ to its potential replacements,
as described in Section~\ref{sec:interface}.

Finally, in order to limit the number of inference rules generated in the
inference projection, we define a restriction function $\zeta$ that specifies,
for a predicate $P \in \predsym{\loglang}^n$, which of the predicates in
$\predsym{\loglang}^n$ may serve as replacements.  Our system uses WordNet
\citep{miller:wordnet2009} to restrict substitutions only to those predicates
representing synonyms or hypernyms of the lemma underlying $P$.  So, for a
predicate $P \in \predsym{\loglang}^n$ and a set of predicates ${\cal Q}
\subseteq \predsym{\loglang}^n$, we define $\zeta$ as \[ \zeta(P,{\cal Q}) = \{
Q \in {\cal Q} ~|~ Q\text{'s lemma is a synonym of, a hypernym of, or equal to
}P\text{'s lemma} \} \]


\subsection*{A lexical ambiguity example}

Assume we have sentence \eqref{ex:lexical-ambiguity}, which is parsed by C\&C
and translated into DRT by Boxer, as shown in Figure
\ref{drs:lexical-ambiguity}.

\begin{covex}\label{ex:lexical-ambiguity}
  A stadium craze is sweeping the country.
\end{covex}

\begin{figure}
  \centering
  ~~~~~~~~
  \subfloat[Dependency output from C\&C]{\label{drs:lexical-ambiguity-deps}
    \begin{minipage}[c][0.7\width]{0.5\textwidth}
	  %\centering
	    \begin{tikzpicture}[level distance=50pt, sibling distance=30pt]
	      \Tree 
	        [.sweep
	          \edge node[auto=right]{ncsubj}; [.craze  
	            \edge node[auto=right]{det};   [.a ]
	            \edge node[auto=left]{ncmod}; [.stadium ]
	          ]
	          \edge node[auto=right]{aux}; [.is ]
	          \edge node[auto=left]{dobj}; [.country 
	            \edge node[auto=left]{det}; [.the ]
	          ]
	        ]
	    \end{tikzpicture}
		% (ncmod _ craze_2 stadium_1)
		% (det craze_2 A_0)
		% (det country_6 the_5)
		% (dobj sweeping_4 country_6)
		% (aux sweeping_4 is_3)
		% (ncsubj sweeping_4 craze_2 _)
	\end{minipage}
  }
  \subfloat[DRT output from Boxer]{\label{drs:lexical-ambiguity-drs}
    \begin{minipage}[c][0.7\width]{0.5\textwidth}
	  %\centering
		\drs{~x0 x1 e2 x3~}{
		  ~\pred{stadium}{1002}(x0)~ \\
		  ~nn(x0, x1)~ \\
		  ~\pred{craze}{1003}(x1)~ \\
		  ~agent(e2, x1)~ \\
		  ~\pred{sweep}{1005}(e2)~ \\
		  ~event(e2)~ \\
		  ~\pred{country}{1007}(x3)~ \\
		  ~patient(e2, x3)~
		}
	\end{minipage}
  }
  \caption{Dependency parse tree and DRT interpretation of
  sentence \eqref{ex:lexical-ambiguity}}
  \label{drs:lexical-ambiguity}
\end{figure}

The DRS in Figure \ref{drs:lexical-ambiguity-drs}, a formula of logical language
\loglang, shall be denoted by $G$.  Formula $G$ contains a unary predicate
$sweep_{1005}$.  In order to generate weighted substitution rules for
$sweep_{1005}$, we calculate the {\it contextualized inference projection} of
$sweep_{1005}$: the set of inference rules mapping $sweep_{1005}$ to each
(unary) predicate $Q \in \predsym{\loglang}^1$, with each rule weighted by the
similarity of the vector representing the context of $sweep_{1005}$ in $G$ to
the vector representing the replacement $Q$. Since the unary predicate  occurs
in exactly one formula, $G$, we calculate the contextualized inference
projection as
\begin{align*}
&\Pi^G_{\simfunc, \zeta, \ell}(sweep_{1005}) = \\
& \hspace{50px} \{ (F, \eta) \mid \exists Q \in \zeta(P, \predsym{\loglang}^1)~[ \\
& \hspace{110px} F = \forall x.[sweep_{1005}(x) \to Q(x)] \text{ and } \\
& \hspace{110px} \eta = \simfunc\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big)~] \}
\end{align*}

Let us assume that our logical language \loglang also includes unary predicates
$cover_{2007}$ and $brush_{3004}$ and that the lemmas {\it cover} and {\it
brush} are known to be synonyms of {\it sweep} (though from different senses). 
In other words, \[ \{ cover_{2007},~ brush_{3004} \} \in \zeta(sweep_{1005},~
\predsym{\loglang}^1) \] So, in the calculation of $\Pi^G_{\simfunc,\zeta
\ell}(sweep_{1005})$, we will generate weighted inference rules $(F,\eta)$ for
both $cover_{2007}$ and $brush_{3004}$.

We look first at $cover_{2007}$.  The rule formula $F$ is instantiated simply as
\[ \forall x.[sweep_{1005}(x) \to cover_{2007}(x)] \]  The weight $\eta$ is the
similarity between the context of $sweep_{1005}$ in $G$, and $cover_{2007}$.
The context vector for $sweep_{1005}$ is calculated as \[
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)) \]  Since we defined the
lexical mapping $\ell(P)$ to simply return the vector from $V$ for the lemma
portion of the predicate $P$, $\ell(sweep_{1005}) = \vv{sweep}$ and
$\ell(cover_{2007}) = \vv{cover}$.  

The context of $P$ in $G$, $\kappa(P,G)$ is the set of a set of
predicates and their relations to $P$, so
\begin{align*}
\kappa(sweep_{1005}, G) 
= \{ & (\ell(\pred{stadium}{1002}),~same\text{-}sentence) \} \\
     & (\ell(\pred{craze}{1003}),~same\text{-}sentence), \\ 
     & (\ell(\pred{country}{1007}),~same\text{-}sentence), \\ 
= \{ & (\vv{stadium},~same\text{-}sentence), \\
     & (\vv{craze},~same\text{-}sentence), \\
     & (\vv{country},~same\text{-}sentence) \}
\end{align*}
We defined our contextualization function $\alpha(\vec v, c)$ to be the vector
sum of word vectors from the context $c$, so
\begin{align*}
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G))
& = \alpha(\vv{sweep},~ \{ 
                (\vv{stadium},~same\text{-}sentence), \\
& \hspace{60px} (\vv{craze},~same\text{-}sentence), \\
& \hspace{60px} (\vv{country},~same\text{-}sentence) \}) \\
& = \vv{stadium} + \vv{craze} + \vv{country}
\end{align*}

Finally, since we have the vector representing the context of $sweep_{1005}$ in
$G$ and the vector representing the replacement predicate $cover_{2007}$, we can
compute the weight, $\eta$ for our inference rule $\forall x.[sweep_{1005}(x)
\to cover_{2007}(x)]$ as
\begin{align*}
& \simfunc\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big) \\
& \hspace{120px} = \simfunc\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{cover}\big) \\
& \hspace{120px} = cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{cover}\big)
\end{align*}
Likewise, the rule for replacing $sweep_{1005}$ by $brush_{3004}$ would be 
$\forall x.[sweep_{1005}(x)$ $\to$ $brush_{3004}(x)]$ weighted by 
$cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{brush}\big)$.

Since, $cosine\big(\vv{stadium}$ $+$ $\vv{craze}$ $+$ $\vv{country},~
\vv{cover}\big)$ $>$ $cosine\big(\vv{stadium}$ $+$ $\vv{craze}$ $+$ $\vv{country},~
\vv{brush}\big)$, {\it cover} is considered to be a better replacement for {\it
sweep} than {\it brush} in the sentence ``A stadium craze is sweeping the
country''.  Thus, the rule $\forall x.[sweep_{1005}(x) \to cover_{2007}(x)]$
will be given more consideration during inference.


% \subsection*{Hypernymy}
% 
% \entpairex{ex:hyp-0}{Ed owns a car.}{Ed has a automobile.}
% 	
% Even restricting rules to wordnet relationships, \eqref{ex:hyp-0} works in
% because car and automobile are synonyms (in the same synset).
% We construct rule 
% 
% $(w,~ car \to automobile)$ where w = $sim(ctx(car),~ automobile)$
% 
% \entpairex{ex:hyp-1}{Ed owns a car.}{Ed has a vehicle.}
% 
% Even restricting rules to wordnet relationships, this works because vehicle
% is a hypernym of (a sense of) car.
% We can construct a rule 
% 
% $(w,~ car \to vehicle)$ where w = $sim(ctx(car),~ vehicle)$
% 
% \entpairex{ex:hyp-2}{Ed owns a vehicle.}{Ed has a car.}
% 
% This is a less likely, but not altogether terrible entailment.
% However, using wordnet strictly would fail since vehicle is a hypernym of (a
% sense of) car.
% So, instead of strictly limiting our rules to wordnet relationships, we could
% have separate rules with separate weights:
% 
% $(w_1,~ vehicle \to car)$ where $w_1 = sim(ctx(vehicle), car)$ 
% 
% $(w_2,~ vehicle \to \lnot car)$ where $w_2$ = a measure of how far {\it car} and
% {\it vehicle} are in the hierarchy.
% 
% So, we can say that, with certainty w1, that 'car' fits into 'vehicle''s context
% based on distributional data.
% Further, we can say with certainly w2 that 'vehicle' can be replaced with 'car'
% based on ontology data.
% 
% 
% 
% \subsection*{Integration between logical and distributional phenomena}
% 
% 
% 
% Some natural language phenomena are most naturally treated as categorial, while
% others are more naturally treated using weights or probabilities. In this paper,
% we treat implicativity, while using a probabilistic approach to word meaning.
% 
% 
% 
% For example \eqref{ex:ws-imp-1}, ``fail to'' is a negatively entailing
% implicative in a positive environment.
% So, $p$ correctly entails $h_{good}$ in both the theorem prover and Alchemy. 
% However, the theorem prover incorrectly licenses the entailment of $h_{bad}$
% while Alchemy does not.  
% The probabilistic approach performs better in this situation because the
% categorial approach does not distinguish between a good paraphrase and a
% bad one.  This example also demonstrates the advantage of using a
% context-sensitive distributional model to calculate the probabilities of
% paraphrases because ``reward'' is an {\it a priori} better paraphrase than
% ``observe'' according to WordNet since it appears in a higher ranked synset. 
% 
% \begin{covex}\label{ex:ws-imp-1}
% \begin{itemize}
%   \item[$p$:] The U.S. is watching closely as South Korea fails to honor
%   U.S. patents\footnote{Example \eqref{ex:ws-imp-1} is adapted from Penn
%   Treebank document wsj\_0020 while \eqref{ex:ws-imp-2} is adapted from document
%   wsj\_2358}
%   \item[$h_{good}$:] South Korea does not {\bf observe} U.S. patents
%   \item[$h_{bad}$:] South Korea does not {\bf reward} U.S. patents
% \end{itemize}
% \end{covex}
% 
% 
% 
% 
% Above, we stated that the entailment in \eqref{ex:hyp-1} was licensed because a
% {\it car} is a type of {\it vehicle} and we can entail from a subset to a
% superset.  In fact, the situation is complicated a bit because the direction of
% entailment is actually dictated by the polarity of the context in which the
% words appear.
% 
% Consider example \eqref{ex:hyp-3} below
% \entpairex{ex:hyp-3}{Ed does not own a vehicle.}{Ed does not have a car.}
% This entailment is valid despite the fact that we are entailing from {\it
% vehicle} to {\it car}, the opposite direction as in example \eqref{ex:hyp-1}. 
% The difference is that in \eqref{ex:hyp-1}, the words appeared in a {\it
% positive context} while in \eqref{ex:hyp-3} they appear in a {\it
% negative context} since they are embedded under a single negation.
% 
% However, the approach that we have chosen to take handles this interaction
% naturally.  The softened inference rules we generate for hypernym relationships
% may lower the probability of entailment versus a similar hard rule (when the
% weight is less than 1), but if an entailment rule does not fit the polarity of
% the context, then it will not raise the probability of entailment.
% 
% In addition to negation, other linguistic constructs such as quantifiers and
% implicative verbs may affect the polarity of a context
% \citep{maccartney:iwcs2009}.  Our system handles all equally well.
% 
