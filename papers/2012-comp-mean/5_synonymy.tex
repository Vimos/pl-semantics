\section{Ambiguity in word meaning}

[TODO: Explicitly mention excusion of meta-predicates: agent, patient, etc]

In order for our system to be able to make correct natural language inferences,
it must be able to handle paraphrasing.  For example, in order to license the
entailment pair in (\ref{ex:syn-hyp-pos}), the system must recognize that
``owns'' is a valid paraphrase for ``has'', and that a ``car'' is type of
``vehicle''.

\begin{example}\label{ex:syn-hyp-pos}
$p:$ Ed owns a car. \\
$h:$ Ed has a vehicle.
\end{example}

Perhaps the most natural fit for our system of projecting distributional
information into logical forms is trying to generate inference rules to address
lexical ambiguity.  For any natural language sentence $A$, a word $v$ in $A$ may
be replaced by a synonym of $v$, $w$, resulting in a new sentence $A'$.  The
degree to which $A'$ means the same thing as $A$ is determined by how well $w$
fits the context of $v$ in $A$.  Thus, in order to capture the probability that
$A$ entails $A'$, we want to generate an inference rule stating that $v$ implies
$w$ to the degree that that $w$ fits the context of $v$.

We can address this problem using the formalism introduced in
Section \ref{sec:interface}.

First, we generate a vector space $V$.  The vector space is generated from a
corpus in which all sentences have been lemmatized with the same lemmatizer used
by Boxer.  This ensures that the predicates output by Boxer will match the
points captured in the vector space.  The features used by $V$ are the N most
frequent dependency-relation/lemma pairs (ignoring stopwords).  Example features
might include $(has\text{-}subject,$ $dog)$ for which {\it fetch} might have a
high count, or $(object\text{-}of,$ $drive)$ for which {\it car} might be
frequent.  Each lemma in the corpus is represented by a vector in $V$.  To calculate these points, we
count the number of times the lemma appears in the same sentence each feature,
and then calculate the point-wise mutual information (PMI) between the lemma and
each feature.  The resulting PMI values are used as the vector for the lemma.
[TODO: Improve.]

Since we are concerned with measuring the similarity between points in our
vector space $V$, we require a {\it similarity function} $S$.  We define $S$
over vectors $\vec v$ and $\vec w$ to be to be simple cosine similarity \[
S(\vec v, \vec w) = cosine(\vec v, \vec w) = \frac{\vec v \cdot \vec w}{||\vec
v||~||\vec w||}\]

Logical forms in our system are generated by Boxer, so our logical language
\loglang is the set of formulas that may be returned from Boxer.  Likewise, the
set of predicate symbols $\predsym{\loglang}$ are the predicates generated by
Boxer. Boxer's predicates, as represented by the {\tt pred} relation in Boxer's
Prolog output\footnote{See
\url{http://svn.ask.it.usyd.edu.au/trac/candc/wiki/DRSs} for the detailed
grammar of Boxer DRS output.}, consist of a word lemma and a token index
indicating the original token that generated that predicate.  As such, our 
\emph{lexical mapping} $\ell$ simply returns the lemma portion of the predicate.

In order to assess the similarity between a word's context and a possible
replacement word, we must define a \textit{context mapping} that generates a
context from a predicate $P \in \predsym{\loglang}$ and a formula $G \in
\loglang$.  Since every predicate in a logical form returned by Boxer is indexed
with the sentence from which it was generated, we can define a simple context
mapping that defines a predicate's context solely in terms of the other
predicates generated by boxer for that sentence.
\begin{align*}
\kappa(P,G) = \{ (dummy\text{-}relation, \ell(Q)) ~|~ 
&~Q \text{ is a predicate found in } G, \\
&~Q\text{'s sentence index} = P\text{'s sentence index}, \text{ and } \\
&~Q \neq P \}
\end{align*}
As an alternative context mapping, we might want to take into
account the relation between predicates $P$ and $Q$.  To do this, we would
combine the output of Boxer with the corresponding output of the C\&C parser,
which contains dependency information.  Using the index information for each
predicate in the Boxer output, we can retrieve the relation connecting $P$ and
$Q$ and use it in a revised context mapping
\begin{align*}
\kappa'(P,G) = \{ (r_i, \ell(Q)) ~|~ 
&~Q \text{ is a predicate found in } G, \\
&~r_i = \text{the relation connecting } P \text{ and } Q, \text{ and } \\
&~Q \neq P \}
\end{align*}

Since the context mapping $\kappa$ returns a set as context, we require a
\textit{contextualization function} that converts this set into a vector in $V$.
Because our vector space is defined simply over lemmas, our contextualization
function just adds the vectors for each lemma in the context \[ \alpha(\vec v,
c) = \sum_{(r_i, \vec w_i) \in c} \vec w_i \]  Thus, the vector $\alpha(\vec v,
c)$ representing the context of word $v$ is simply the sum of vectors for the
words in that context.
[TODO: Again, we don't use \vec v.  Also: how can we modify $V$ to be able to
use $r_i$?]

Based on these definitions, we can compute the \textit{contextualized
substitution projection} $\Pi'_{S, \ell}(P)$, the set of weighted inference
rules mapping predicate $P$ to its potential replacements.


\subsection*{A lexical ambiguity example}

Assume we have sentence \eqref{ex:lexical-ambiguity}

\begin{example}\label{ex:lexical-ambiguity}
  A stadium craze is sweeping the country.
\end{example}

which is parsed by C\&C and translated into DRT by Boxer, as shown in Figure
\ref{drs:lexical-ambiguity}.

\begin{figure}
  \centering
  \subfloat[Dependency output from C\&C]{\label{drs:lexical-ambiguity-deps}
    \begin{tikzpicture}[level distance=50pt, sibling distance=30pt]
      \Tree 
        [.sweep
          \edge node[auto=right]{ncsubj}; [.craze  
            \edge node[auto=right]{det};   [.a ]
            \edge node[auto=left]{ncmod}; [.stadium ]
          ]
          \edge node[auto=right]{aux}; [.is ]
          \edge node[auto=left]{dobj}; [.country 
            \edge node[auto=left]{det}; [.the ]
          ]
        ]
    \end{tikzpicture}
	% (ncmod _ craze_2 stadium_1)
	% (det craze_2 A_0)
	% (det country_6 the_5)
	% (dobj sweeping_4 country_6)
	% (aux sweeping_4 is_3)
	% (ncsubj sweeping_4 craze_2 _)
  }
  ~~~~~~~~~
  \subfloat[DRT output from Boxer]{\label{drs:lexical-ambiguity-drs}
    ~~~~~~~~~~
	\drs{~x0 x1 x2 e3~}{
	  ~country_{1007}(x0)~ \\
	  ~stadium_{1002}(x1)~ \\
	  ~craze_{1003}(x2)~ \\
	  ~nn(x1, x2)~ \\
	  ~sweep_{1005}(e3)~ \\
	  ~event(e3)~ \\
	  ~agent(e3, x2)~ \\
	  ~patient(e3, x0)~
	}
	~~~~~~~~~~
  }
  \caption{Parse tree and DRT interpretation of \eqref{ex:lexical-ambiguity}}
  \label{drs:lexical-ambiguity}
\end{figure}

The DRS in Figure \ref{drs:lexical-ambiguity-drs}, a formula of logical language
\loglang, shall be denoted by $G$.  Formula $G$ contains a unary predicate
$sweep_{1005}$.  In order to generate weighted substitution rules for
$sweep_{1005}$, we calculate the {\it contextualized substitution projection} of
$sweep_{1005}$: the set of inference rules mapping $sweep_{1005}$ to each
(unary) predicate $Q \in \predsym{\loglang}^1$, with each rule weighted by the
similarity of the vector representing the context of $sweep_{1005}$ in $G$
to the vector representing the replacement $Q$\footnote{In reality, one might
limit the space of possible inference rules by using a resource such as
WordNet to ensure that the lemma of $Q$ is a synonym (or other closely related
word) of the target word {\it sweep}.}.
Since the unary predicate  occurs in exactly one formula, $G$, we calculate the 
contextualized substitution projection as 
\begin{align*}
\Pi'_{S, \ell}(sweep_{1005}) =  
\{ (F, \eta) \mid \exists Q \in \predsym{\loglang}^1~[ 
&~F = \forall x.[sweep_{1005}(x) \to Q(x)], \\
&~\eta = S\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big), \text{ and } \\
&~ \eta > 0 ~] \}
\end{align*}

Let us assume that our logical language \loglang also includes unary predicates
$cover_{2007}$ and $brush_{3004}$.  In other words, $\{ cover_{2007},~
brush_{3004} \} \in \predsym{\loglang}^1$.
So, in the calculation of $\Pi'_{S, \ell}(sweep_{1005})$, we will generate
weighted inference rules $(F,\eta)$ for both $cover_{2007}$ and $brush_{3004}$.

We look first at $cover_{2007}$.  The rule formula $F$ is instantiated simply as
$\forall x.[sweep_{1005}(x) \to cover_{2007}(x)]$.  The weight $\eta$ is the
similarity between the context of $sweep_{1005}$ in $G$, and $cover_{2007}$.
The context vector for $sweep_{1005}$ is calculated as \[
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)) \]  Since we defined the
lexical mapping $\ell(P)$ to simply return the vector from $V$ for the lemma
portion of the predicate $P$, $\ell(sweep_{1005}) = \vec{sweep}$ and
$\ell(cover_{2007}) = \vec{cover}$.  

The context of $P$ in $G$, $\kappa(P,G)$, uses the dependency parse information
found in Figure \ref{drs:lexical-ambiguity-deps} to construct a set of
predicates and their relations to $P$, so
\begin{align*}
\kappa(sweep_{1005}, G)
&= \{ (has\text{-}subject,~\ell(craze_{1003})),~ (has\text{-}object,~\ell(country_{1007})) \} \\ 
&= \{ (has\text{-}subject,~\vec{craze}),~ (has\text{-}object,~\vec{country}) \}
\end{align*}
We defined our contextualization function $\alpha(\vec v, c)$ to be the vector
sum of word vectors from the context $c$, so
\begin{align*}
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G))
&= \alpha(\vec{sweep},~ \{ (has\text{-}subject,~\vec{craze}),~ (has\text{-}object,~\vec{country}) \}) \\
&= \vec{craze} + \vec{country}
\end{align*}

Finally, since we have the vector representing the context of $sweep_{1005}$ in
$G$ and the vector representing the replacement predicate $cover_{2007}$, we can
compute the weight, $\eta$ for our inference rule $\forall x.[sweep_{1005}(x)
\to cover_{2007}(x)]$ as
\begin{align*}
S\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big)
&= S\big(\vec{craze} + \vec{country},~ \vec{cover}\big) \\
&= cosine\big(\vec{craze} + \vec{country},~ \vec{cover}\big)
\end{align*}

Likewise, the rule for replacing $sweep_{1005}$ by $brush_{3004}$ would be 
$\forall x.[sweep_{1005}(x) \to brush_{3004}(x)]$, weighted by 
$cosine\big(\vec{craze} + \vec{country},~ \vec{brush}\big)$.

Since, $cosine\big(\vec{craze} + \vec{country},~ \vec{cover}\big) > 
cosine\big(\vec{craze} + \vec{country},~ \vec{brush}\big)$, {\it cover} is
considered to be a better replacement for {\it sweep} than {\it brush} in the
sentence ``A stadium craze is sweeping the country''.  Thus, the rule 
$\forall x.[sweep_{1005}(x) \to cover_{2007}(x)]$ will be given more
consideration during inference.


\subsection*{Hypernymy}

\begin{example}\label{ex:hyp-0}
$p:$ Ed owns a car. \\
$h:$ Ed has a automobile.
\end{example}

Even restricting rules to wordnet relationships, this works because car and
automobile are synonyms (in the same synset).
We construct rule 

$(w,~ car \to automobile)$ where w = $sim(ctx(car),~ automobile)$


\begin{example}\label{ex:hyp-1}
$p:$ Ed owns a car. \\
$h:$ Ed has a vehicle.
\end{example}

Even restricting rules to wordnet relionships, this works because vehicle is a
hypernym of (a sense of) car.
We can construct a rule 

$(w,~ car \to vehicle)$ where w = $sim(ctx(car),~ vehicle)$

\begin{example}\label{ex:hyp-2}
$p:$ Ed has a vehicle. \\
$h:$ Ed owns a car.
\end{example}

This is a less likely, but not altogether terrible entailment.
However, using wordnet strictly would fail since vehicle is a hypernym of (a
sense of) car.
So, instead of strictly limiting our rules to wordnet relationships, we could
have separate rules with separate weights:

$(w_1,~ vehicle \to car)$ where $w_1 = sim(ctx(vehicle), car)$ 

$(w_2,~ vehicle \to \lnot car)$ where $w_2$ = a measure of how far {\it car} and
{\it vehicle} are in the hierarchy.

So, we can say that, with certainty w1, that 'car' fits into 'vehicle''s context
based on distributional data.
Further, we can say with certainly w2 that 'vehicle' can be replaced with 'car'
based on ontology data.



\subsection*{Integration between logical and distributional phenomena}

Above, we stated that the entailment in \eqref{ex:hyp-1} was licensed because a
{\it car} is a type of {\it vehicle} and we can entail from a subset to a
superset.  In fact, the situation is complicated a bit because the direction of
entailment is actually dictated by the polarity of the context in which the
words appear.

Consider example \eqref{ex:hyp-3} below
\begin{example}\label{ex:hyp-3}
$p:$ Ed does not own a vehicle. \\
$h:$ Ed does not have a car.
\end{example}
This entailment is valid despite the fact that we are entailing from {\it
vehicle} to {\it car}, the opposite direction as in example \eqref{ex:hyp-1}. 
The difference is that in \eqref{ex:hyp-1}, the words appeared in a {\it
positive context} while in \eqref{ex:hyp-3} they appear in a {\it
negative context} since they are embedded under a single negation.

However, the approach that we have chosen to take handles this interaction
naturally.  The softened inference rules we generate for hypernym relationships
may lower the probability of entailment versus a similar hard rule (when the
weight is less than 1), but if an entailment rule does not fit the polarity of
the context, then it will not raise the probability of entailment.

In addition to negation, other linguistic constructs such as quantifiers and
implicative verbs may affect the polarity of a context
\citep{maccartney:iwcs2009}.  Our system handles all equally well.

