\section{Ambiguity in word meaning}

In order for our system to be able to make correct natural language inferences,
it must be able to handle paraphrasing.  For example, in order to license the
entailment pair in (\ref{ex:syn-hyp-pos}), the system must recognize that
``owns'' is a valid paraphrase for ``has'', and that a ``car'' is type of
``vehicle''.

\entpairex{ex:syn-hyp-pos}{Ed owns a car.}{Ed has a vehicle.}

Perhaps the most natural fit for our system of projecting distributional
information into logical forms is trying to generate inference rules to address
lexical ambiguity.  For any natural language sentence $A$, a word $v$ in $A$ may
be replaced by a synonym of $v$, $w$, resulting in a new sentence $A'$.  The
degree to which $A'$ means the same thing as $A$ is determined by how well $w$
fits the context of $v$ in $A$.  Thus, in order to capture the probability that
$A$ entails $A'$, we want to generate an inference rule stating that $v$ implies
$w$ to the degree that that $w$ fits the context of $v$.

We can address this problem using the formalism introduced in
Section \ref{sec:interface}.

First, we generate a vector space $V$.  We have chosen to implement a very
simple bag-of-words vector space.  To ensure that the entries in the vector
space correspond to the predicates in our logical forms, we first lemmatize all
sentences in our corpus using the same lemmatization process as Boxer.
The features used by $V$ are the $N$ most frequent lemmas, excluding stopwords.  
Each lemma in the corpus is represented by a vector in $V$.  To calculate these
vectors, we count the number of times the lemma appears in the same sentence
as each feature, and then calculate the point-wise mutual information (PMI)
between the lemma and each feature.  The resulting PMI values are used as the 
vector for the lemma.

Since we are concerned with measuring the similarity between points in our
vector space $V$, we require a {\it similarity function} $S$.  We define $S$
over vectors $\vec v$ and $\vec w$ to be to be simple cosine similarity \[
S(\vec v, \vec w) = cosine(\vec v, \vec w) = \frac{\vec v \cdot \vec w}{\|\vec
v\|~\|\vec w\|}\]

Logical forms in our system are generated by Boxer, so our logical language
\loglang is the set of formulas that may be returned from Boxer.  Likewise, the
set of predicate symbols $\predsym{\loglang}$ are the predicates generated by
Boxer. Boxer's predicates, as represented by the {\tt pred} relation in Boxer's
Prolog output\footnote{See
\url{http://svn.ask.it.usyd.edu.au/trac/candc/wiki/DRSs} for the detailed
grammar of Boxer DRS output.}, consist of a word lemma and a token index
indicating the original token that generated that predicate.  As such, our 
\emph{lexical mapping} $\ell$ simply returns the lemma portion of the predicate.

In order to assess the similarity between a word's context and a possible
replacement word, we must define a \textit{context mapping} that generates a
context from a predicate $P \in \predsym{\loglang}$ and a formula $G \in
\loglang$.  
We can define our context mapping as a function that maps $P$ to the set of
vectors representing the other predicates in $G$ along with the relations that
connect them.
\begin{align*}
\kappa(P,G) = \{ (r_i, \ell(Q)) ~|~ 
&~Q \text{ is a predicate found in } G, \\
&~r_i = \text{the relation connecting } P \text{ and } Q, \text{ and } \\
&~Q \neq P \}
\end{align*}
However, in our current scenario, the only ``relation'' we use is the relation
of being in the same sentence.  So, we are defining the context of $P$ as the
vectors of all predicates $Q$ that occur in the same sentence as $P$.
Since every predicate in a logical form returned by Boxer is indexed
with the sentence from which it was generated, we can define a simple context
mapping that defines a predicate's context solely in terms of the other
predicates generated by boxer for that sentence.
\begin{align*}
\kappa'(P,G) = \{ (same\text{-}sentence, \ell(Q)) ~|~ 
&~Q \text{ is a predicate found in } G, \\
&~Q\text{'s sentence index} = P\text{'s sentence index}, \text{ and } \\
&~Q \neq P \}
\end{align*}
Note that the only predicates $Q$ that are used are those derived from the
lemmas of words found in the text.  Meta-predicates representing relations such
as $agent$, $patient$, and $theme$ are not included.

Since the context mapping $\kappa$ returns a set as context, we require a
\textit{contextualization function} that converts this set into a vector in $V$.
Because our vector space is defined simply over lemmas, our contextualization
function just adds the vectors for each lemma in the context \[ \alpha(\vec v,
c) = \sum_{(r_i, \vec w_i) \in c} \vec w_i \]  Thus, the vector $\alpha(\vec v,
c)$ representing the context of word $v$ is simply the sum of vectors for the
words in that context.

Based on these definitions, we can compute the \textit{contextualized
substitution projection} $\Pi^G_{S, \ell}(P)$, the set of weighted inference
rules mapping predicate $P$ to its potential replacements.


\subsection*{A lexical ambiguity example}

Assume we have sentence \eqref{ex:lexical-ambiguity}, which is parsed by C\&C
and translated into DRT by Boxer, as shown in Figure
\ref{drs:lexical-ambiguity}.

\begin{covex}\label{ex:lexical-ambiguity}
  A stadium craze is sweeping the country.
\end{covex}

\begin{figure}
  \centering
  ~~~~~~~~
  \subfloat[Dependency output from C\&C]{\label{drs:lexical-ambiguity-deps}
    \begin{minipage}[c][0.7\width]{0.5\textwidth}
	  %\centering
	    \begin{tikzpicture}[level distance=50pt, sibling distance=30pt]
	      \Tree 
	        [.sweep
	          \edge node[auto=right]{ncsubj}; [.craze  
	            \edge node[auto=right]{det};   [.a ]
	            \edge node[auto=left]{ncmod}; [.stadium ]
	          ]
	          \edge node[auto=right]{aux}; [.is ]
	          \edge node[auto=left]{dobj}; [.country 
	            \edge node[auto=left]{det}; [.the ]
	          ]
	        ]
	    \end{tikzpicture}
		% (ncmod _ craze_2 stadium_1)
		% (det craze_2 A_0)
		% (det country_6 the_5)
		% (dobj sweeping_4 country_6)
		% (aux sweeping_4 is_3)
		% (ncsubj sweeping_4 craze_2 _)
	\end{minipage}
  }
  \subfloat[DRT output from Boxer]{\label{drs:lexical-ambiguity-drs}
    \begin{minipage}[c][0.7\width]{0.5\textwidth}
	  %\centering
		\drs{~x0 x1 e2 x3~}{
		  ~\pred{stadium}{1002}(x0)~ \\
		  ~nn(x0, x1)~ \\
		  ~\pred{craze}{1003}(x1)~ \\
		  ~agent(e2, x1)~ \\
		  ~\pred{sweep}{1005}(e2)~ \\
		  ~event(e2)~ \\
		  ~\pred{country}{1007}(x3)~ \\
		  ~patient(e2, x3)~
		}
	\end{minipage}
  }
  \caption{Dependency parse tree and DRT interpretation of
  sentence \eqref{ex:lexical-ambiguity}}
  \label{drs:lexical-ambiguity}
\end{figure}

The DRS in Figure \ref{drs:lexical-ambiguity-drs}, a formula of logical language
\loglang, shall be denoted by $G$.  Formula $G$ contains a unary predicate
$sweep_{1005}$.  In order to generate weighted substitution rules for
$sweep_{1005}$, we calculate the {\it contextualized substitution projection} of
$sweep_{1005}$: the set of inference rules mapping $sweep_{1005}$ to each
(unary) predicate $Q \in \predsym{\loglang}^1$, with each rule weighted by the
similarity of the vector representing the context of $sweep_{1005}$ in $G$
to the vector representing the replacement $Q$\footnote{In reality, one might
limit the space of possible inference rules by using a resource such as
WordNet to ensure that the lemma of $Q$ is a synonym (or other closely related
word) of the source word {\it sweep}.}.
Since the unary predicate  occurs in exactly one formula, $G$, we calculate the 
contextualized substitution projection as 
\begin{align*}
&\Pi'_{S, \ell}(sweep_{1005}) = \\
& \hspace{50px} \{ (F, \eta) \mid \exists Q \in \predsym{\loglang}^1~[ \\
& \hspace{110px} F = \forall x.[sweep_{1005}(x) \to Q(x)] \text{ and } \\
& \hspace{110px} \eta = S\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big)~] \}
\end{align*}

Let us assume that our logical language \loglang also includes unary predicates
$cover_{2007}$ and $brush_{3004}$.  In other words, $\{ cover_{2007},~
brush_{3004} \} \in \predsym{\loglang}^1$.
So, in the calculation of $\Pi'_{S, \ell}(sweep_{1005})$, we will generate
weighted inference rules $(F,\eta)$ for both $cover_{2007}$ and $brush_{3004}$.

We look first at $cover_{2007}$.  The rule formula $F$ is instantiated simply as
\[ \forall x.[sweep_{1005}(x) \to cover_{2007}(x)] \]  The weight $\eta$ is the
similarity between the context of $sweep_{1005}$ in $G$, and $cover_{2007}$.
The context vector for $sweep_{1005}$ is calculated as \[
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)) \]  Since we defined the
lexical mapping $\ell(P)$ to simply return the vector from $V$ for the lemma
portion of the predicate $P$, $\ell(sweep_{1005}) = \vv{sweep}$ and
$\ell(cover_{2007}) = \vv{cover}$.  

The context of $P$ in $G$, $\kappa(P,G)$, uses the dependency parse information
found in Figure \ref{drs:lexical-ambiguity-deps} to construct a set of
predicates and their relations to $P$, so
\begin{align*}
\kappa(sweep_{1005}, G) 
= \{ & (same\text{-}sentence,~\ell(\pred{stadium}{1002})) \} \\
     & (same\text{-}sentence,~\ell(\pred{craze}{1003})), \\ 
     & (same\text{-}sentence,~\ell(\pred{country}{1007})), \\ 
= \{ & (same\text{-}sentence,~\vv{stadium}), \\
     & (same\text{-}sentence,~\vv{craze}), \\
     & (same\text{-}sentence,~\vv{country}) \}
\end{align*}
We defined our contextualization function $\alpha(\vec v, c)$ to be the vector
sum of word vectors from the context $c$, so
\begin{align*}
\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G))
& = \alpha(\vv{sweep},~ \{ 
                (same\text{-}sentence,~\vv{stadium}), \\
& \hspace{60px} (same\text{-}sentence,~\vv{craze}), \\
& \hspace{60px} (same\text{-}sentence,~\vv{country}) \}) \\
& = \vv{stadium} + \vv{craze} + \vv{country}
\end{align*}

Finally, since we have the vector representing the context of $sweep_{1005}$ in
$G$ and the vector representing the replacement predicate $cover_{2007}$, we can
compute the weight, $\eta$ for our inference rule $\forall x.[sweep_{1005}(x)
\to cover_{2007}(x)]$ as
\begin{align*}
& S\big(\alpha(\ell(sweep_{1005}), \kappa(sweep_{1005}, G)), \ell(Q)\big) \\
& \hspace{120px} = S\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{cover}\big) \\
& \hspace{120px} = cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{cover}\big)
\end{align*}
Likewise, the rule for replacing $sweep_{1005}$ by $brush_{3004}$ would be 
\[ \forall x.[sweep_{1005}(x) \to brush_{3004}(x)] \], weighted by 
$cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~ \vv{brush}\big)$.

Since, $cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~
\vv{cover}\big) > cosine\big(\vv{stadium} + \vv{craze} + \vv{country},~
\vv{brush}\big)$, {\it cover} is considered to be a better replacement for {\it
sweep} than {\it brush} in the sentence ``A stadium craze is sweeping the
country''.  Thus, the rule $\forall x.[sweep_{1005}(x) \to cover_{2007}(x)]$
will be given more consideration during inference.


\subsection*{Hypernymy}

\entpairex{ex:hyp-0}{Ed owns a car.}{Ed has a automobile.}
	
Even restricting rules to wordnet relationships, \eqref{ex:hyp-0} works in
because car and automobile are synonyms (in the same synset).
We construct rule 

$(w,~ car \to automobile)$ where w = $sim(ctx(car),~ automobile)$

\entpairex{ex:hyp-1}{Ed owns a car.}{Ed has a vehicle.}

Even restricting rules to wordnet relationships, this works because vehicle
is a hypernym of (a sense of) car.
We can construct a rule 

$(w,~ car \to vehicle)$ where w = $sim(ctx(car),~ vehicle)$

\entpairex{ex:hyp-2}{Ed owns a vehicle.}{Ed has a car.}

This is a less likely, but not altogether terrible entailment.
However, using wordnet strictly would fail since vehicle is a hypernym of (a
sense of) car.
So, instead of strictly limiting our rules to wordnet relationships, we could
have separate rules with separate weights:

$(w_1,~ vehicle \to car)$ where $w_1 = sim(ctx(vehicle), car)$ 

$(w_2,~ vehicle \to \lnot car)$ where $w_2$ = a measure of how far {\it car} and
{\it vehicle} are in the hierarchy.

So, we can say that, with certainty w1, that 'car' fits into 'vehicle''s context
based on distributional data.
Further, we can say with certainly w2 that 'vehicle' can be replaced with 'car'
based on ontology data.



\subsection*{Integration between logical and distributional phenomena}



Some natural language phenomena are most naturally treated as categorial, while
others are more naturally treated using weights or probabilities. In this paper,
we treat implicativity, while using a probabilistic approach to word meaning.



For example \eqref{ex:ws-imp-1}, ``fail to'' is a negatively entailing
implicative in a positive environment.
So, $p$ correctly entails $h_{good}$ in both the theorem prover and Alchemy. 
However, the theorem prover incorrectly licenses the entailment of $h_{bad}$
while Alchemy does not.  
The probabilistic approach performs better in this situation because the
categorial approach does not distinguish between a good paraphrase and a
bad one.  This example also demonstrates the advantage of using a
context-sensitive distributional model to calculate the probabilities of
paraphrases because ``reward'' is an {\it a priori} better paraphrase than
``observe'' according to WordNet since it appears in a higher ranked synset. 

\begin{covex}\label{ex:ws-imp-1}
\begin{itemize}
  \item[$p$:] The U.S. is watching closely as South Korea fails to honor
  U.S. patents\footnote{Example \eqref{ex:ws-imp-1} is adapted from Penn
  Treebank document wsj\_0020 while \eqref{ex:ws-imp-2} is adapted from document
  wsj\_2358}
  \item[$h_{good}$:] South Korea does not {\bf observe} U.S. patents
  \item[$h_{bad}$:] South Korea does not {\bf reward} U.S. patents
\end{itemize}
\end{covex}




Above, we stated that the entailment in \eqref{ex:hyp-1} was licensed because a
{\it car} is a type of {\it vehicle} and we can entail from a subset to a
superset.  In fact, the situation is complicated a bit because the direction of
entailment is actually dictated by the polarity of the context in which the
words appear.

Consider example \eqref{ex:hyp-3} below
\entpairex{ex:hyp-3}{Ed does not own a vehicle.}{Ed does not have a car.}
This entailment is valid despite the fact that we are entailing from {\it
vehicle} to {\it car}, the opposite direction as in example \eqref{ex:hyp-1}. 
The difference is that in \eqref{ex:hyp-1}, the words appeared in a {\it
positive context} while in \eqref{ex:hyp-3} they appear in a {\it
negative context} since they are embedded under a single negation.

However, the approach that we have chosen to take handles this interaction
naturally.  The softened inference rules we generate for hypernym relationships
may lower the probability of entailment versus a similar hard rule (when the
weight is less than 1), but if an entailment rule does not fit the polarity of
the context, then it will not raise the probability of entailment.

In addition to negation, other linguistic constructs such as quantifiers and
implicative verbs may affect the polarity of a context
\citep{maccartney:iwcs2009}.  Our system handles all equally well.

