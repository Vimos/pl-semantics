\section{Future work}

Or plans for continued word can be divided into two categories: word on the
theoretical side and work on implementation.

From a theoretical perspective, we would like to extend our formalism to address
a wider range of linguistic phenomena.  A huge amount of natural language is
probabilistic in nature, and first-order representations do not correctly
address these problems.  By extending our framework, we hope to be able to
apply weights derived from distributional information to a wide variety of
modeled concepts.

We would also like to enhance the approaches that we have presented here.  For
example, our procedure for handling word meaning ambiguity generates a weight
based only on the words that appear in the same sentence as the target.  It
would be more interesting to extract relations from the logical form to create a
more interesting $\alpha$ function.

% [think about vector spaces that take logical form into account similar to how
% pado and lapata took dependencies into account.]

From an implementation perspective, we would like to produce a dataset that
tests all of the issues outlined in this paper, and to run a large-scale
evaluation of our techniques.  
However, the major barrier here is that the Alchemy software has severe
inefficiencies in terms of memory requirements and speed, preventing us
from executing larger and more complex examples.  There is on-going
work to improve Alchemy [TODO: CITE Vibhav's PTP paper].

We are also interested in comparing our approaches to recent work on building
vectors from entire sentences [TODO: CITATIONS].  We would like to example
how our approach compares in the task of measuring similarity between
phrases.


\section{Conclusion}

In this paper, we have introduced a formalism that allows us to integrating
logical semantic representations with probabilistic weights acquired from
distributional vector spaces.  We showed how these weighted first-order
representations can be used to perform probabilistic first-order inferences
using Markov Logic.  We have shown how our approach handles three distinct
phenomena, word meaning ambiguity, hypernymy, and implicativity, as well as
allowing them to interact appropriately.  Most importantly our approach allows 
us to model discrete logical phenomena with hard first-order techniques and
probabilistic phenomena with soft weights, and to do all of them within a
single, unified framework.
The resulting approach is able to correctly solve a number of difficult
textual entailment problems that require handling complex combinations of these
important semantic phenomena.

% The framework we have developed takes a pair of natural
% language sentences as input and parses them into DRS \citep{kamp:book93}
% representations.  It then augments those representations by linking the
% predicates back to the original words in the sentences and incorporating
% coreference information from OntoNotes \citep{hovy:naacl2006}.
% Since DRSs are hierarchical structures, our approach flattens them to a simple
% list of atoms while keeping track of the original structure through DRS labels
% as arguments, allowing inferences to be performed in 
% an MLN. In order to maintain the DRS semantics in the flat logical form, we have 
% hand-written a collection of inference rules that are used in the inference. 
% We also generate a list of inference rules to address the particular
% linguistic phenomena that we are handling.  Categorial rules based
% on implication signatures \citep{nairn:icos2006} are used to handle
% implicativity and factivity.  Weighted rules are used to address issues of
% word meaning in context.



\section*{Acknowledgements}

This work was supported by the Department of Defense (DoD) through a
National Defense Science and Engineering Graduate Fellowship (NDSEG) Fellowship
for the first author, National Science Foundation grant IIS-0845925 for the
second author, and a grant from the Longhorn Innovation Fund for Technology.
[TODO: Is all of this right?]
