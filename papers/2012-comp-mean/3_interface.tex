% logical language
\newcommand{\loglang}{\ensuremath{{\cal{L}}}\xspace}
% predicate symbols
\newcommand{\predsym}[1]{\ensuremath{{\cal{P}}_{#1}}\xspace}
% similarity function
\newcommand{\simfunc}{\ensuremath{\mathrm{sim}}}

\section{Linking logical form and vector spaces}
\label{sec:interface}

In this section we define a link between logical form and vector space
representations through a mapping function that connects predicates in
logical form to points in vector space. \citet{Gardenfors:04} uses the
interpretation function for this purpose, such that 
logical formulas are interpreted over vector space
representations. However, he uses spaces whose dimensions are
qualities, like the hue and saturation of a color or the taste of a
fruit. Points in his conceptual spaces are, therefore, potential
entities. In contrast, the vector spaces that we use are
distributional in nature. A point in such a space is a potential word,
defined through its observed contexts. For this reason, we define the link between logical form
and vector space through a second mapping function independent of the
interpretation function, which we call the \emph{lexical mapping}
function. 

\subsection*{Lexical mapping and inference projection} 

% [TODO: \\vec is making vectors bold instead of using an overarrow]
% KE: leave them bold. this is due to mathptmx.
% Probably too much of a nuisance to change back, plus it 
% might be inconsistent with other papers in the book to have an overarrow.

Let $V$ be a vector space whose dimensions stand for elements of  textual
context. We also write $V$ for the set of points in the space. We assume that each word is represented as a point in vector
space.~\footnote{The assumption of a single vector per word is made
  for the sake of simplicity. If we want to cover models in which each word is
  represented through multiple
  vectors~\citep{ReisingerMooney:10,dinu-lapata:2010:EMNLP}, this can
  be done through straightforward extensions of the definitions given here.} The central relation in vector spaces is semantic
similarity. We represent this through a \textit{similarity function} \[\simfunc: V
\times V \to [0,1] \] that maps each pair of points in vector space to their
degree of similarity. While most similarity functions in the
literature are symmetric, such that  $\simfunc(\vec v, \vec w) = \simfunc(\vec w,
\vec v)$, our definition also accommodates asymmmetric similarity
measures like \citet{kotlerman:nlej2010}. 

We link logical form and a vector space through a function that maps every predicate symbol to a point in
space. Let \loglang be a logical language. For each $n \ge
0$, let the set of $n$-ary predicate symbols of \loglang be
$\predsym{\loglang}^n$, and let $\predsym{\loglang} = \cup_{n \ge 0}
\predsym{\loglang}^n$. Let $V$ be a vector space. Then a \emph{lexical
  mapping function}
from \loglang to $V$ is a function $\ell:
\predsym{\loglang} \to V$.

A central property
of distributional vector spaces is that they can predict similarity in
meaning based on similarity in observed
contexts~\citep{Harris}. \citet{LinPantel:01} point out that
vector space similarity can be expressed in the form of 
inference rules: If two words $v$
and $w$ are similar in their observed contexts, we can write this as
the inference rule $v \to w$, which states that $w$ can be substituted
for $v$ in texts. The inference rule is weighted by $\simfunc(\vec v, \vec w)$. 

We use this same idea to project inference rules from vector space to
logical form through the lexical mapping function. If the lexical mapping
function maps the n-ary predicate $P$ to $\vec
v$ and the n-ary predicate $Q$ to $ \vec w$, and $\simfunc(\vec v, \vec w) = \eta$, then
we obtain the weighted inference rule $\forall x_1,
\ldots, x_n[ P(x_1, \ldots, x_n) \to Q(x_1, \ldots x_n) ]$ with weight
$\eta$. More generally, let \loglang
be a logical language with lexical mapping $\ell$ to a vector space 
$V$. Let \simfunc{} be the similarity function on $V$. For all $Q \in
\predsym\loglang$ and ${\cal Q} \subseteq \predsym\loglang$, let
$\zeta(Q, {\cal Q}) \subseteq {\cal Q}$. Then the \emph{inference
  projection} for the predicate $P \in \predsym{\loglang}^n$ is 
\begin{align*}
\Pi_{\simfunc, \zeta, \ell}(P) = \{ (F, \eta) \mid \exists Q \in \zeta(P, \predsym{\loglang}^n)\:[ 
&~F = \forall x_1, \ldots, x_n[P(x_1, \ldots, x_n) \to Q(x_1, \ldots, x_n)], \\
&~\eta = \simfunc\big(\ell(P), \ell(Q)\big) ~] \}
\end{align*}

The inference projection for $P$ contains all weighted inference rules
$(F, \eta)$ with $P$ on the left-hand side that the vector space
predicts. However, sometimes we have additional information not
encoded in the vector space on the inferences that we are
willing to project. For example we may only want to consider
predicates $Q$ that stand for paraphrases of $P$. For this reason, the
function $\zeta$ can limit the predicates $Q$ considered for the right-hand
sides of rules to a subset of $\predsym{\loglang}^n$. 



\subsection*{Addressing polysemy}

We follow the literature on vector space approaches to modeling word
meaning in
context~\citep{erk:emnlp08,ThaterFuerstenauPinkal:10,ReisingerMooney:10,dinu-lapata:2010:EMNLP,vandecruys:emnlp2011}
in assuming that a word's context-specific meaning is a function of its
out-of-context representation and the context. The context may consist
of a single item or multiple items, and (syntactic or semantic) relations to the
target word may also play a
role~\citep{erk:emnlp08,ThaterFuerstenauPinkal:10,vandecruys:emnlp2011}. 

We
first define what we mean by a context. Given a vector space $V$ and a
finite set $R$ of semantic relations, the set
\textit{$C(V, R)$ of contexts over $V$ and $R$} consists of all finite sets of
pairs from $R \times V$.  That is, we describe the context in which a
target word occurs as a finite set of pairs $(\vec v, r)$ of a context
item $\vec v$ represented as a point in vector space, and the relation $r$
between the context item and the target. 
For a word $v$ in a context $c
\in C(V, R)$, the context-specific meaning $\vec w_c$ of $w$ is a
function of the out-of-context vector $\vec w$ for $w$ and the context
$c$:
\[\vec w_c = \alpha(\vec w, c)\]
The function $\alpha$ is a \emph{contextualization function} with
signature $\alpha: V \times C(V, R) \to V$. 

This definition of contextualization functions is similar to the
framework of \citet{MitchellLapata:08}, wh define the meaning $\vec p$
of a two-word phrase $vw$ as a function of the vectors for $v$, $w$, and their
syntactic relation: $\vec p = f(\vec v, \vec w, r, K)$,  where $f$ is some function, $r$ is the
relation between $v$ and $w$ in the text, and $K$ is background
knowledge. However, we use contextualization functions to compute the meaning of a word in
context, rather than the meaning of a phrase: Our aim is to define a
mapping from the predicate symbols of a logical language to a vector
space, and predicate symbols need to map to word meanings, not phrase
meanings. Also, Mitchell and Lapata only consider the case of two-word
phrases, while we allow for arbitrary-size contexts. 


% Proximity in a
% vector space that is distributional in nature represents
% substitutability in context. For that reason, the similarity function can be understood as a
% substitutability function.  If words $v$ and $w$ are represented by $\vec
% v$ and $\vec w \in V$ and $\simfunc(\vec v, \vec w) = \eta$, then $w$ can be
% substituted for $v$ to the degree $\eta$.

Vector space approaches to representing word meaning in context in the
literature are typically evaluated on a paraphrasing task: Given a
word $v$ in a sentence context $c$, how appropriate is a paraphrase
candidate 
$w$ for $v$ in this context? The appropriateness of $w$ is typically
predicted as 
\[ \mathrm{appl}(w) = \simfunc(\alpha(\vec v, c), \vec w)\] 
That is, the appropriateness of $w$ is predicted as the similarity of
the vector for $w$ to the context-specific vector for $v$. This
formulation adapts $v$ to the context $c$, but leaves the vector $w$  
unchanged~\citep{erk:emnlp08,MitchellLapata:08,ThaterFuerstenauPinkal:10,vandecruys:emnlp2011}.
If the paraphrase candidate $w$ is itself ambiguous, it can be useful
to adapt its vector to the context $c$ as
well~\citep{erk:acl2010}. In that case, the appropriateness of $w$ is
predicted as 
\[ \mathrm{appl}(w) = \simfunc(\alpha(\vec v, c), \alpha(\vec w, c)) \]




However, since we are interested in the substitutability of words {\it in
context}, we compute context-specific lexical mappings by first computing a
context from a logical form. Given a logical language \loglang, a vector space
$V$, and set $R$ of semantic relations, a \textit{context mapping} is a function
\[ \kappa: \predsym{\loglang} \times \loglang \to C(V, R) \] Given a predicate
$P \in \predsym{\loglang}$ and a formula $G \in \loglang$, it computes a context
$c = \kappa(P, G)$.

By combining context mappings with contextualization functions, we can now
describe how we extend a logical form by context-specific inferences: Let
\loglang be a logical language with lexical mapping $\ell$ to vector space $V$.
Let \simfunc{} be a similarity function on $V$, $\alpha$ a contextualization function
on $V$ and $R$, and $\kappa$ a context mapping from \loglang to $C(V, R)$.
Then the \textit{contextualized substitution projection} for predicate $P \in
\predsym{\loglang}^n$ found in formula $G \in \loglang$ is
\begin{align*}
\Pi^G_{\simfunc, \ell}(P) = \{ (F, \eta) \mid \exists Q \in \predsym{\loglang}^n~[ 
&~F = \forall x_1, \ldots, x_n.[P(x_1, \ldots, x_n) \to Q(x_1, \ldots, x_n)], \\
&~\eta = \simfunc\big(\alpha(\ell(P), \kappa(P, G)), \ell(Q)\big) ~] \}
\end{align*}
This ensures that similarity is measured between the replacement $Q$ the {\em contextualized}
vector representing $P$.

Thus, the aggregate contextualized substitution projection for an entire formula
$G$ is the union of the contextualized substitution projections for all
predicates in $G$

\[\Pi^*_{\simfunc, \ell}(G) = \bigcup_{~P \in \predsym{\loglang} \text{ occurs in }
G} \Pi^G_{\simfunc, \ell}(P) \] 

% [TODO: Might be clearer to explicitly say:] Thus, the contextualized
% substitution projection is given by
% \begin{align*}
% \Pi'_{\simfunc, \ell}(P) =  \{ (F, \eta) \mid \exists Q \in \predsym{\loglang}^n~[ 
% &~F = \forall x_1, \ldots, x_n.P(x_1, \ldots, x_n) \to Q(x_1, \ldots, x_n), \\
% &~\eta = \simfunc\big(\alpha(\ell(P), \kappa(P, G)), \ell(Q)\big), \text{ and } \\
% &~ \eta > 0 ~] \}
% \end{align*}

This formalization only contextualizes $P$ and estimates the substitutability of
$Q$ based on a context-independent vector. If we like,we can substitute a
different lexical mapping that maps both $P$ and $Q$ to context-specific
vectors: Given a context-independent lexical mapping $\ell$, contextualization
function $\alpha$ and context mapping $\kappa$, a predicate $P \in
\predsym{\loglang}$ and formula $G \in \loglang$, let $\gamma^{P, G, \alpha,
\kappa}$ be the lexical mapping defined as \[\gamma^{P, G, \ell, \alpha,
\kappa}(Q) = \alpha\big(\ell(Q), \kappa(P, G)\big) \] Then we can compute the
aggregate contextualized substitution projection for $G$ as \[\Pi^*_{\simfunc, \ell}(G)
= \bigcup_{P \in \predsym{\loglang} \text{ occurs in } G} \Pi_{\simfunc, \gamma^{P, G, \ell, \alpha,
\kappa}}(P) \]

