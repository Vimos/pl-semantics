\section{Background}

\textbf{Recognizing textual entailment.}
Textual entailment is the task of determining whether one natural language text
implies another.  We have chosen textual entailment as the mode of evaluation
for our approach because it offers a good framework for testing whether a system
performs correct analyses and thus draws the right inferences from a given text.
% For example, to test whether a system correctly handles implicative verbs, one
% can use the \emph{premise} $p$ along with the \emph{hypothesis} $h$ in
% \eqref{ex:imp-fact-nested} below. If the system analyses the two sentences
% correctly, it should infer that $h$ holds.
While the most prominent forum using textual entailment is the Recognizing
Textual Entailment (RTE) challenge \citep{dagan:rte2005}, the RTE datasets do
not test the phenomena in which we are interested. For example, in order to
evaluate our system's ability to determine word meaning in context, the RTE pair
would have to specifically test word sense confusion by having a word's context
in the hypothesis be different from the context of the premise.  However, this
simply does not occur in the RTE corpora.  In order to properly test our
phenomena, we construct hand-tailored premises and hypotheses based on
real-world texts.


\textbf{Logic-based semantics.}
Boxer \citep{bos:coling2004} is a software package for wide-coverage semantic
analysis that provides semantic representations in the form of Discourse
Representation Structures \citep{kamp:book93}. It builds on the C\&C CCG parser
\citep{clark:acl04}.
\citet{bos:emnlp2005} describe a system for Recognizing Textual Entailment
(RTE) that uses Boxer to convert both the premise and hypothesis of an RTE pair
into first-order logical semantic representations and then uses a theorem prover
to check for logical entailment. 
% \citet{bos:trec2006} varies this model in order
% to use Boxer in a question answering setting by using Boxer to generate a
% logical representation of a document and a question and attempting to unify the
% two to find an answer to the question.


\noindent\textbf{Distributional models for lexical meaning.} Distributional
models describe the meaning of a word through the context in which it
appears~\citep{landauer97:solution,lund96:producing}, where contexts can be
documents, other words, or snippets of syntactic structure. Distributional
models are able to predict semantic similarity between words based on
distributional similarity and they can be learned in an unsupervised fashion.
Recently distributional models have been used to predict the applicability of
paraphrases in context \citep{erk:emnlp2008,thater:acl2010,reisinger:naacl2010,dinu:emnlp2010,vandecruys:emnlp2011}.
For example, in ``The wine left a stain'', {\it result in} is a better
paraphrase for {\it leave} than is {\it entrust}, while the opposite is true in
``He left the children with the nurse''. Usually, the distributional
representation for a word mixes all its usages (senses). For the paraphrase
appropriateness task, these representations are then reweighted, extended, or
filtered to focus on contextually appropriate usages.

\noindent\textbf{Markov Logic.} 
In order to perform logical inference with weights, we draw
from the large and active body of work related to Statistical Relational AI
\citep{getoor:book2007}.  Specifically, we make use of Markov Logic Networks
(MLNs) \citep{richardson:mlj06} which employ weighted graphical models to
represent first-order logical formulas. MLNs are appropriate for our approach
because they provide an elegant method of assigning weights to first-order
logical rules, combining a diverse set of inference rules, and performing
probabilistic inference.

An MLN consists of a set of weighted first-order clauses.  It provides a way of
softening first-order logic by making situations in which not all clauses are
satisfied less likely, but not impossible \citep{richardson:mlj06}. More
formally, if $X$ is the set of all propositions describing a world (i.e. the
set of all ground atoms), $\mathcal{F}$ is the set of all clauses in the MLN,
$w_i$ is the weight associated with clause $f_i \in \mathcal{F}$,
$\mathcal{G}_{f_i}$ is the set of all possible groundings of clause $f_i$, and
$\mathcal{Z}$ is the normalization constant, then the probability of a
particular truth assignment $\mathbf{x}$ to the variables in $X$ is defined as:
\[ P(X = \mathbf{x}) = \frac{1}{\mathcal{Z}} \exp\left(\sum_{f_i \in
\mathcal{F}} w_i \sum_{g \in \mathcal{G}_{f_i}}g(\mathbf{x}) \right) =
\frac{1}{\mathcal{Z}} \exp\left(\sum_{f_i \in \mathcal{F}} w_i n_i(\mathbf{x})
\right) \tag{1}\label{e1} \] where $g(\mathbf{x})$ is 1 if $g$ is satisfied and
0 otherwise, and $n_i(\mathbf{x})= \sum_{g\in \mathcal{G}_{f_i}}g(\mathbf{x})$
is the number of groundings of $f_i$ that are satisfied given the current truth
assignment to the variables in $X$. This means that the probability of a truth
assignment rises exponentially with the number of groundings that are satisfied.

Markov Logic has been used previously in other NLP application
(e.g. \citet{poon:emnlp2009}).  However, this paper marks the first attempt at
representing deep logical semantics in an MLN.

While it is possible learn rule weights in an MLN directly from training data,
our approach at this time focuses on incorporating weights computed
by external knowledge sources.  Weights for word meaning rules are computed from
the distributional model of lexical meaning and then injected into the MLN. 
Rules governing implicativity and coreference are given infinite weight
(hard constraints).

We use the open source software package Alchemy \citep{kok:tr05} to perform MLN
inference.
